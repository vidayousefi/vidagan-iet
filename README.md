# Official code for paper: VidaGAN: Adaptive GAN for image steganography

# Abstrct:
A recent approach to image steganography is to use deep learning. Mainly, convolutional neural networks can extract complex features and use them as patterns to combine hidden messages and images. Also, by using generative adversarial networks, it is possible to generate realistic and high-quality stego images without any noticeable artifacts. Previous methods suffered from challenges such as simple architecture, low network accuracy, imbalance between capacity and transparency, vanishing gradients, and low capacity. This study introduces a steganography framework named VidaGAN that utilizes deep learning techniques. The network being proposed is made up of three components: an encoder, a decoder, and a critic, and introduces a novel architecture and several innovations to address some of the unresolved challenges mentioned above. This study introduces a novel method for embedding any type of binary data into images using generative adversarial networks, enabling us to enhance the visual appeal of images generated by the specified model. This neural network called VarIable aDAptive GAN (VidaGAN) achieved state-of-the-art status by reaching a hiding capacity of 3.9 bits per pixel in the DIV2K dataset. Furthermore, examination by the StegExpose steganalysis tool shows an AUC of 0.6, a suitable threshold for transparency.

## Citation

If you use this code or repository in your research, please cite the following paper:

**Vida Yousefi Ramandi, Mansoor Fateh, Mohsen Rezvani.** "VidaGAN: Adaptive GAN for image steganography." *IET Image Processing*, vol. 18, no. 12, pp. 3329â€“3342, 2024. DOI: [10.1049/ipr2.13177](https://doi.org/10.1049/ipr2.13177)

### BibTeX
```bibtex
@article{yousefi2024vidagan,
  title = {VidaGAN: Adaptive GAN for image steganography},
  author = {Yousefi Ramandi, Vida and Fateh, Mansoor and Rezvani, Mohsen},
  journal = {IET Image Processing},
  volume = {18},
  number = {12},
  pages = {3329--3342},
  year = {2024},
  doi = {10.1049/ipr2.13177},
  url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.13177}
}
